{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "## Reference\n",
    "\n",
    "Abu-Mostafa, Learning from Data\n",
    "\n",
    "## Definitions\n",
    "\n",
    "(Mitchell, 1997) A machine *learns* if its *performance* (error) in executing some *task* (processing) gets batter with its *experience* (data).\n",
    "\n",
    "Machine learning applies to problems in which there exists a *pattern*.\n",
    "\n",
    "##### Learning Problem\n",
    "\n",
    "A learning problem consists of:\n",
    "\n",
    "- an *unknown function* $f: X \\rightarrow Y$ that takes an *input* set $X$ to an *output* set $Y$\n",
    "- a *data set* $D = \\{(x_i, y_i) \\in X \\times Y\\ :\\ y_i = f(x_i)\\}$\n",
    "- a *hypothesis set* of functions, $H$\n",
    "- a *learning algorithm* $A$ that uses $D$ to pick $g \\in H$ that best approximates $f$\n",
    "\n",
    "We call $\\{H, A\\}$ the *learning model*.\n",
    "\n",
    "##### Examples\n",
    "\n",
    "1. The input set $X$ can be a collection of fundamentals of some company, i.e., revenue, earnings, assets, liabilities, growth etc., and the output set $Y$ is its stock price \n",
    "(the month average). We might want to learn the function $f$ (if there is any) which predict the stock price based on the fundamentals.\n",
    "\n",
    "2. Our input set is the price variation of a stock in the last $n$ days, that is, $X=\\mathbb{R}^n$, and the output set is $\\{-1, 1\\}$, where $-1$ indicates that the stock will go down \n",
    "tomorrow, and \n",
    "$1$ that it will go up.\n",
    "\n",
    "\n",
    "## Classes of ML algorithms\n",
    "\n",
    "Supervised Learning $\\rightarrow D = \\{(\\text{input, output})\\}$\n",
    "\n",
    "Reinforcement Learning $\\rightarrow D = \\{(\\text{input, some output, output's score})\\}$\n",
    "\n",
    "Unsupervised Learning $\\rightarrow D = \\{(\\text{input})\\}$\n",
    "\n",
    "Here we deal mainly with supervised learning.\n",
    "\n",
    "The concept of \"learning from data\" englobes *machine learning*, which goal is to uncover patterns, *statistics*, tha aims to infere probability distributions from data and *data mining*, that is, data analysis. All three are closely related, but different.\n",
    "\n",
    "\n",
    "## Linear Classification Model: The Perceptron\n",
    "\n",
    "In the linear classification learning model, the hypothesis set consists of all functions\n",
    "\n",
    "$$ h(x) = sgn(w \\cdot x + b) $$\n",
    "\n",
    "with $w \\in \\mathbb{R}^n$. If we set $w_0 = b$ and $x_0 = 1$,\n",
    "\n",
    "$$ h(x) = sgn(w^T\\cdot x) $$\n",
    "\n",
    "with $x \\in X = \\{1\\} \\times \\mathbb{R}^d$, where $d$ is the dimension of the input space.\n",
    "\n",
    "We say that the input space is *linearly separable* if there exists $h$ such that $h(x_i)=y_i$ for any $(x_i, y_i) \\in D$.\n",
    "\n",
    "One example of a linear classification model is the Perceptron Learning Algorithm (PLA), described below:\n",
    "\n",
    "While there is any element in the data set at time $t$ such that $h(x(t)) \\neq y(t)$, update $w$ according to the rule\n",
    "\n",
    "$$ w(t+1) = w(t) + y(t)x(t) $$\n",
    "\n",
    "## Linear Models\n",
    "\n",
    "##### 1) Linear Classification Model\n",
    "The input space is $\\mathbb{R}^d$ and the output space is $Y = \\{+1, -1\\}$\n",
    "$$H = \\{h(x) = sgn(w\\cdot x) : w \\in \\mathbb{R}^{d+1} \\}$$\n",
    "where we set $X = \\{1\\} \\times \\R^d$\n",
    "\n",
    "Example: PLA\n",
    "\n",
    "Application: Handwritten digit recognition, credit approval\n",
    "\n",
    "\"Pocket PLA\": at every iteration, runs $w(t+1)$ over all $D$ and only updates $w(t)$ if it actually got better (more complex!)\n",
    "\n",
    "##### 2) Linear Regression Model\n",
    "\n",
    "Instead of output space $Y = \\{+1, -1\\}$, we let $Y = \\mathbb{R}$ and instead of $y = f(x)$, we have an *unknown target distribution* $P(x,y)$ that generates the data set points. The hypothesis set becomes $H = \\{h(x) = sgn(w\\cdot x) : w \\in \\mathbb{R}^{d+1} \\}$\n",
    "\n",
    "\"Linear\" here means that we assume that $P$ can be approximated by a linear function of the input.\n",
    "\n",
    "Example: Linear Regression Algorithm (LRA)\n",
    "\n",
    "We start by defining an error function\n",
    "\n",
    "$$ E(w) = \\frac{1}{N} \\sum_{n=1}^{N} (w \\cdot x_n - y_n)^2 $$\n",
    "\n",
    "where $(x_n,y_n) \\in D$ and $N$ is the size of $D$. We define the error this way because this function is always $\\geq 0$ and differentiable. But we could choose another $E(w)$ depending on the problem.\n",
    "\n",
    "Then we define $\\mathcal{X} = \\mathbb{R}^{N\\times(d+1)}$ and $\\mathcal{Y} \\in \\mathbb{R^N}$:\n",
    "\n",
    "$$ \\mathcal{X} = \\begin{pmatrix}- x_1 - \\\\ - x_2 -  \\\\ .\\\\.\\\\.\\\\- x_N - \\end{pmatrix} $$\n",
    "\n",
    "$$ \\mathcal{Y} =  \\begin{pmatrix} y_1 \\\\ y_2 \\\\ .\\\\.\\\\.\\\\ y_N \\end{pmatrix} $$\n",
    "\n",
    "Then $$E(w) = \\frac{1}{N}|| \\mathcal{X}w - \\mathcal{Y} ||^2$$\n",
    "\n",
    "The linear regression consists in minimizing this function, that is, finding\n",
    "\n",
    "$$ w^* = \\text{argmin}_{w \\in \\mathbb{R}^{d+1}} E(w) $$\n",
    "\n",
    "As shown in the reference, this has the following analytical solution:\n",
    "\n",
    "$$ w^{*} = \\mathcal{X}^\\dagger b $$\n",
    "$$ \\mathcal{X}^\\dagger = (\\mathcal{X}^T\\mathcal{X})^{-1}\\mathcal{X}^T $$\n",
    "\n",
    "where $\\mathcal{X}^\\dagger$ is called the \"pseudo-inverse\". Notice that linear regression can be used for classification:\n",
    "\n",
    "Make $Y = \\{+1, -1\\} \\rightarrow \\mathbb{R}$. Use linear regression so that $w\\cdot x_i \\approx y_i = \\pm 1$. Then $h(x_i)$ is likely to agree with $y_i$. This is useful for obtaining a good estimate for an initial $w$ in the PLA.\n",
    "\n",
    "## Reference\n",
    "\n",
    "Abu-Mostafa, Learning from Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
