{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "## Reference\n",
    "\n",
    "Abu-Mostafa, Learning from Data\n",
    "\n",
    "## Definitions\n",
    "\n",
    "(Mitchell, 1997) A machine **learns** if its **performance** (error) in executing some **task** (processing) gets better with its **experience** (data).\n",
    "\n",
    "Machine learning applies to problems in which there exists a **pattern**.\n",
    "\n",
    "##### Learning Problem\n",
    "\n",
    "A learning problem consists of:\n",
    "\n",
    "- an *unknown function* $f: X \\rightarrow Y$ that takes an *input* set $X$ to an *output* set $Y$\n",
    "- a *data set* $D = \\{(x_i, y_i) \\in X \\times Y\\ :\\ y_i = f(x_i)\\}$\n",
    "- a *hypothesis set* of functions, $H$\n",
    "- a *learning algorithm* $A$ that uses $D$ to pick $g \\in H$ that best approximates $f$\n",
    "\n",
    "We call $\\{H, A\\}$ the *learning model*.\n",
    "\n",
    "##### Examples\n",
    "\n",
    "1. The input set $X$ can be a collection of fundamentals of some company, i.e., revenue, earnings, assets, liabilities, growth etc., and the output set $Y$ is its stock price \n",
    "(the monthlyaverage). We might want to learn the function $f$ (if there is any) which predict the stock price based on the fundamentals.\n",
    "\n",
    "2. Our input set is the price variation of a stock in the last $n$ days, that is, $X=\\mathbb{R}^n$, and the output set is $\\{-1, 1\\}$, where $-1$ indicates that the stock will go down \n",
    "tomorrow, and \n",
    "$1$ that it will go up.\n",
    "\n",
    "\n",
    "## Classes of ML algorithms\n",
    "\n",
    "Supervised Learning $\\rightarrow D = \\{(\\text{input, output})\\}$\n",
    "\n",
    "Reinforcement Learning $\\rightarrow D = \\{(\\text{input, some output, output's score})\\}$\n",
    "\n",
    "Unsupervised Learning $\\rightarrow D = \\{(\\text{input})\\}$\n",
    "\n",
    "Here we deal mainly with supervised learning.\n",
    "\n",
    "The concept of \"learning from data\" englobes *machine learning*, which goal is to uncover patterns, *statistics*, that aims to infere probability distributions from data and *data mining*, that is, data analysis. All three are closely related, but different.\n",
    "\n",
    "\n",
    "## Linear Classification Model: The Perceptron\n",
    "\n",
    "In the linear classification learning model, the hypothesis set consists of all functions\n",
    "\n",
    "$$ h(x) = sgn(w \\cdot x + b) $$\n",
    "\n",
    "with $w \\in \\mathbb{R}^n$. If we set $w_0 = b$ and $x_0 = 1$,\n",
    "\n",
    "$$ h(x) = sgn(w\\cdot x) $$\n",
    "\n",
    "with $x \\in X = \\{1\\} \\times \\mathbb{R}^d$, where $d$ is the dimension of the input space.\n",
    "\n",
    "We say that the input space is *linearly separable* if there exists $h$ such that $h(x_i)=y_i$ for any $(x_i, y_i) \\in D$.\n",
    "\n",
    "One example of a linear classification model is the Perceptron Learning Algorithm (PLA), described below:\n",
    "~~~\n",
    "While there is any element in the data set at time t such that h(x(t)) != y(t):\n",
    "w(t+1) = w(t) + y(t)x(t)\n",
    "~~~\n",
    "\n",
    "## Linear Models\n",
    "\n",
    "##### 1) Linear Classification Model\n",
    "The input space is $\\mathbb{R}^d$ and the output space is $Y = \\{+1, -1\\}$\n",
    "$$H = \\{h(x) = sgn(w\\cdot x) : w \\in \\mathbb{R}^{d+1} \\}$$\n",
    "where we set $X = \\{1\\} \\times \\mathbb{R}^d$\n",
    "\n",
    "Example: PLA\n",
    "\n",
    "Application: Handwritten digit recognition, credit approval\n",
    "\n",
    "\"Pocket PLA\": at every iteration, runs $w(t+1)$ over all $D$ and only updates $w(t)$ if it actually got better (if the error got smaller) (more complex!)\n",
    "\n",
    "##### 2) Linear Regression Model\n",
    "\n",
    "Instead of output space $Y = \\{+1, -1\\}$, we let $Y = \\mathbb{R}$ and instead of $y = f(x)$, we have an unknown **target distribution** $P(x,y)$ that generates the data set points. The hypothesis set becomes $H = \\{h(x) = w\\cdot x : w \\in \\mathbb{R}^{d+1} \\}$\n",
    "\n",
    "\"Linear\" here means that we assume that $P$ can be approximated by a linear function of the input.\n",
    "\n",
    "Example: Linear Regression Algorithm (LRA)\n",
    "\n",
    "We start by defining an error function\n",
    "\n",
    "$$ E(w) = \\frac{1}{N} \\sum_{n=1}^{N} (w \\cdot x_n - y_n)^2 $$\n",
    "\n",
    "where $(x_n,y_n) \\in D$ and $N$ is the size of $D$. We define the error this way because this function is always $\\geq 0$ and differentiable. But we could choose another $E(w)$ depending on the problem.\n",
    "\n",
    "Then we define $\\mathcal{X} \\in \\mathbb{R}^{N\\times(d+1)}$ and $\\mathcal{Y} \\in \\mathbb{R^N}$:\n",
    "\n",
    "$$ \\mathcal{X} = \\begin{pmatrix}- x_1 - \\\\ - x_2 -  \\\\ .\\\\.\\\\.\\\\- x_N - \\end{pmatrix} $$\n",
    "\n",
    "$$ \\mathcal{Y} =  \\begin{pmatrix} y_1 \\\\ y_2 \\\\ .\\\\.\\\\.\\\\ y_N \\end{pmatrix} $$\n",
    "\n",
    "Then $$E(w) = \\frac{1}{N}|| \\mathcal{X}w - \\mathcal{Y} ||^2$$\n",
    "\n",
    "The linear regression consists in minimizing this function, that is, finding\n",
    "\n",
    "$$ w^* = \\text{argmin}_{w \\in \\mathbb{R}^{d+1}} E(w) $$\n",
    "\n",
    "As shown in the reference, this has the following analytical solution:\n",
    "\n",
    "$$ w^{*} = \\mathcal{X}^\\dagger y $$\n",
    "$$ \\mathcal{X}^\\dagger = (\\mathcal{X}^T\\mathcal{X})^{-1}\\mathcal{X}^T $$\n",
    "\n",
    "where $\\mathcal{X}^\\dagger$ is called the \"pseudo-inverse\".\n",
    "\n",
    "##### 3) Logistic Regression Model\n",
    "\n",
    "The output space is $Y = [0,1]$, that is, the output is the probability for a binary event (for example, the probability of a digit being or not '1'). The hypothesis set is \n",
    "\n",
    "$$ H = \\{ h(x) = \\theta(w\\cdot x) : w \\in \\mathbb{R}^{d+1} \\} $$\n",
    "\n",
    "where $\\theta(s) = \\frac{e^s}{1+e^s}$. \n",
    "\n",
    "The target to be learned is the probability of a certain outcome of a binary event:\n",
    "\n",
    "$$ f(x) = P(y=+1| x) \\text{   so that   } P(y|x) = \\begin{cases} f(x)\\text{, for }y=+1\\\\ 1-f(x)\\text{, for }y =-1 \\end{cases}$$\n",
    "\n",
    "To find a learning algorithm, as before, we need to define an error function. In this model, the error function is based on the notion of **likelihood**. Likelihood expresses the probability of obtaining the dataset $D$ from the hypothesis $h(x)$. As\n",
    "\n",
    "$$ P(y|x) = \\begin{cases}h(x)\\text{ , if }y=+1\\\\1 - h(x)\\text{ , if }y=-1\\end{cases} = \\begin{cases}\\theta(w\\cdot x)\\text{ , if }y=+1\\\\\\theta(-w\\cdot x)\\text{ , if }y=-1\\end{cases} = \\theta(y w \\cdot x)$$\n",
    "\n",
    "where we have used $1 - \\theta(s) = \\theta(-s)$. The likelihood is $ \\prod_{i=1}^N P(y_i|x_i) $\n",
    "\n",
    "We wish to minimize the following error function:\n",
    "\n",
    "$$ E(w) = -\\frac{1}{N} \\ln \\Big(\\prod_i P(y_i|x_i)\\Big) = \\frac{1}{N} \\sum_i \\ln \\Big(\\frac{1}{P(y_i|x_i)}\\Big) = \\frac{1}{N} \\sum_i \\ln \\Big(\\frac{1}{\\theta(y_i w \\cdot x_i)}\\Big)$$\n",
    "\n",
    "This is also called the \"maximum likelihood\" method. Substituting the formula for $\\theta$:\n",
    "\n",
    "$$ E(w) = \\frac{1}{N} \\sum_i \\ln\\Big(1 + e^{-y_i w\\cdot x_i}\\Big) $$\n",
    "\n",
    "Notice that this error is small when $y_i w \\cdot x_i >> 0$. Now, instead of making $\\nabla E(w) = 0$, we will use a new algorithm, the Gradient Descent (GD):\n",
    "\n",
    "At each iteration, we update w according to \n",
    "\n",
    "$$ w(t+1) = w(t) - \\eta \\nabla E(w(t)) $$\n",
    "\n",
    "in general we pick $\\eta \\sim 1$. The gradient is\n",
    "\n",
    "$$ \\nabla E (w) = -\\frac{1}{N} \\sum_i \\frac{y_i x_i}{1 + e^{-y_i w \\cdot x_i}} $$\n",
    "\n",
    "The initial weight $w(0)$ can be chosen randomly, and the termination criteria can be set by a maximum value of $t$, a threshold for $||\\nabla E||$ or even for $||E||$ itself.\n",
    "\n",
    "An alternative to GD is the Stochastic Gradient Descent (SGD). In this method, instead of calculating the gradient using all $N$ points of the data set, at each iteration we pick a point $(x_i, y_i)$ uniformly at random and use it to update the weights as\n",
    "\n",
    "$$w(t+1) = w(t) - \\eta \\frac{y_ix_i}{1 + e^{-y_i w \\cdot x_i}}$$\n",
    "\n",
    "This works because the expected value of the gradient change over the a uniform distribution would be $- \\eta\\frac{1}{N}\\sum_i \\frac{y_ix_i}{1 + e^{-y_i w \\cdot x_i}}$ (exactly the same as in GD), so on average the descent proceeds in the right direction.\n",
    "\n",
    "The advantages of SGD over GD are that SGD is computationally cheaper (each step is O(1) vs O(N)), the randomization in SGD makes sure that the descent can escape from shallow local minima, and it is very simple to implement.\n",
    "\n",
    "#### Summary of linear models\n",
    "\n",
    "| ![image](summary.png) |\n",
    "|:--:| \n",
    "| ![image](applications.png) |\n",
    "| Taken from https://work.caltech.edu/lectures.html |\n",
    "\n",
    "Note that both Lin. Reg. and Log. Reg. can be used for classification:\n",
    "\n",
    "Log. Reg. produces $g(x) = P(y=+1|x)$. We can classify $x$ as $+1$ if $P(+1|x) \\geq \\frac{1}{2}$. This corresponds to using $w_{Log.Reg.}$ as $w_{Lin.Clas.}$ and setting $w_0=\\frac{1}{2}$.\n",
    "\n",
    "Lin. Reg. can be used to fit $h(x)$ to $y=\\pm 1$ values such that  if $sgn(w_{Lin.Reg} \\cdot x) = +1$, we clasify as $+1$: make $Y = \\{+1, -1\\} \\rightarrow \\mathbb{R}$. Use linear regression so that $w\\cdot x_i \\approx y_i = \\pm 1$. Then $h(x_i)$ is likely to agree with $y_i$. This is useful for obtaining a good estimate for an initial $w$ in the PLA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feasibility of Learning\n",
    "\n",
    "As long as $f$ is unknown, knowing $D$ cannot exclude any values for $f$ outside $D$. For learning to be feasbile, we need that the **in-sample error** $E_{in}(g)$, that is, the error of $g$ in the points of $D$, equals the **out-of-sample error** $E_{out}(g)$. This is generally impossible, so we require instead that\n",
    "\n",
    "$$ P[|E_{in}(g) - E_{out}(g)| > \\epsilon] $$\n",
    "\n",
    "is bounded:\n",
    "\n",
    "$$ P[|E_{in}(g) - E_{out}(g)| > \\epsilon]  \\leq P[|E_{in}(h_1) - E_{out}(h_1)| > \\epsilon\\ OR\\ |E_{in}(h_2) - E_{out}(h_2)| > \\epsilon\\ OR\\ ....] $$\n",
    "$$ \\leq \\sum_{m=1}^M P[|E_{in}(h_m) - E_{out}(h_m)| > \\epsilon] $$\n",
    "\n",
    "where $M$ is the total number of hypothesis in $H$, which we assume finite. Using the Hoeffding inequality, where $N$ is the number of data points,\n",
    "\n",
    "$$P[|E_{in}(h_m) - E_{out}(h_m)| > \\epsilon] \\leq 2e^{-2\\epsilon^2N}$$\n",
    "\n",
    "So\n",
    "\n",
    "$$P[|E_{in}(g) - E_{out}(g)| > \\epsilon] \\leq 2Me^{-2\\epsilon^2N}$$\n",
    "\n",
    "Therefore for any tolerance $\\epsilon$, there is a large enough $N$ for which the probability is small enough for our purposes. We say that the $g$ is **PAC** (probably approximately correct).\n",
    "\n",
    "One problem with this reasoning is that, generally, $M$ is infinite. We want thus to replace $M$ by a finite quantity. We can do this if we consider **dichotomies** instead of hypothesis (restricting ourselves to the case of linear classification):\n",
    "\n",
    "A hypothesis is a function $h: X \\rightarrow \\{-1, +1\\}$. There are infinitely many such hypothesis $M = |H|$.\n",
    "\n",
    "A dichotomy is a function $h: D \\rightarrow \\{-1,+1\\}$. There are $|H(D)| \\leq 2^N$ such dichotomies: they are a **restriction** of $H$ to $D$.\n",
    "\n",
    "We can now define the **growth function**, which counts the most dichotomies that can exist on $N$ points:\n",
    "\n",
    "$$ m_H(N) = \\max_D |H(x_1,...,x_N)| \\leq 2^N $$\n",
    "\n",
    "If we replace $M$ by $m_H(N)$, we want it to be at most polynomial in $N$, so that it will be crushed by $e^{-2\\epsilon^2N}$.\n",
    "\n",
    "We say that $k$ is a **break point** for $H$ if no data set $D$ of size $k$ can be **shattered** by $H$, which means that \n",
    "\n",
    "$$m_H(N) < 2^k$$\n",
    "\n",
    "(that is, we can't separate $k$ points in every possible way using $H$).\n",
    "\n",
    "For a 2D perceptron, $k = 4$ is a break point.\n",
    "\n",
    "It can be shown that, if there is a break point, $m_H(N)$ is polynomial:\n",
    "\n",
    "$$ m_H(N) \\approx N^{k-1} + 1 $$\n",
    "\n",
    "The growth function accounts for overlaps between elements of $H$ when applied to $D$ (points where $h = h'$): many hypothesis return the same dichotomy. The result of substituting $M$ for the growth function, after some technicalities, is the **generalization bound** (Vapnik-Chervonenkis inequality):\n",
    "\n",
    "$$P[|E_{in}(g) - E_{out}(g)| > \\epsilon] \\leq 4 m_H(2N) e^{-\\frac{1}{8}\\epsilon^2N}$$\n",
    "\n",
    "Another important definition is the **VC dimension**. It is defined as the largest number of points, $$d_{VC}(H)$$, which can be shattered by $H$ (that is, there is **at least one** set $D$ with $d_{VC}$ points which can be shattered). Note that every $k > d_{VC}$ is a break point, and \n",
    "\n",
    "$$ k_{min} = d_{VC} +1 $$\n",
    "\n",
    "For example, $d_{VC}=3$ for the 2D perceptron (although 3 collinear points can't be shattered). For the d-dimensional perceptron, $d_{VC} = d+1$. Note that $d+1$ is also the number of parameters $w_0,...,w_d$. In general, $d_{VC}$ can be interpreted as the **number of degrees of freedom** of the model ($d_{VC}$ is a property of the model). Using the VC inequality and that $m_H(N) \\approx N^{d_{VC}}$, we see if\n",
    "\n",
    "$$P[|E_{in}(g) - E_{out}(g)| > \\epsilon] \\leq \\delta$$\n",
    "\n",
    "then $N$, the number of examples necessary for the model to achieve accuracy $\\epsilon$ with certainty $\\delta$, depends on $d_{VC}$. As a rule of thumb, we can take\n",
    "\n",
    "$$ N \\geq 10 d_{VC} $$\n",
    "\n",
    "Also, we can formulate the generalization bound in a more friendly form. Set\n",
    "\n",
    "$$\\epsilon = \\sqrt{\\frac{8}{N} ln \\frac{4m_H(2N)}{\\delta}} = \\Omega(N, H, \\delta)$$\n",
    "\n",
    "then we can say that, **with probability $\\geq 1-\\delta$, $E_{out}-E_{in} \\leq \\Omega \\Rightarrow E_{out} \\leq E_{in}+\\Omega$**.\n",
    "\n",
    "Note that $\\delta \\uparrow \\Omega \\uparrow, m_H \\uparrow \\Omega \\downarrow $. Also, $N\\uparrow E_{in}\\uparrow\\text{ and }\\Omega\\downarrow$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance analysis\n",
    "\n",
    "VC analysis: $E_{out} \\leq E_{in} + \\Omega$\n",
    "\n",
    "In the Bias-Variance analysis we follow a different approach, but with the same goal of finding how to measure how well a hypothesis performs out-of-sample. Let $g^D$ be the hypothesis produced by some learning model based on a dataset $D$. Then, the average of the out-of-sample error of this hypothesis over all $D$ is:\n",
    "\n",
    "$$ \\langle E_{out}(g^D \\rangle_D = \\langle \\langle (g^D(x) - f(x))^2 \\rangle_x \\rangle_D $$\n",
    "$$ = \\langle \\langle (g^D(x) - f(x))^2 \\rangle_D \\rangle_x$$\n",
    "\n",
    "Define the \"average hypothesis\" (the \"best\" possible hypothesis) $\\bar{g}(x) \\in H$ such that\n",
    "\n",
    "$$ \\bar{g}(x) = \\langle g^D(x) \\rangle_D \\approx \\frac{1}{K} \\sum_{i=1}^K g^{D_i}(x) $$\n",
    "\n",
    "For some collection of data sets $\\{D_1, ..., D_K\\}$. Then \n",
    "\n",
    "$$ \\langle (g^D(x) - f(x))^2\\rangle_D = \\langle (g^D(x) - \\bar{g}(x) + \\bar{g}(x) - f(x))^2\\rangle_D $$\n",
    "$$ = \\langle (g^D(x) - \\bar{g}(x))^2 + (\\bar{g}(x) - f(x))^2 + 2(g^D(x)-\\bar{g}(x)(\\bar{g}(x) - f(x)))\\rangle_D $$\n",
    "\n",
    "Using $\\langle g^D(x) - \\bar{g}(x) \\rangle_D = 0$ we arrive at\n",
    "\n",
    "$$ \\langle (g^D(x) - f(x))^2\\rangle_D = \\langle (g^D(x) - \\bar{g}(x))^2\\rangle_D + \\langle(\\bar{g}(x) - f(x))^2\\rangle_D $$\n",
    "$$ = \\text{var}(x) + \\text{bias}(x)$$\n",
    "\n",
    "Note that $\\text{var}(x) = \\langle (g^D(x) - \\bar{g}(x))^2\\rangle_D$ is the \"error\" due to the fact that $D$ is finite, and $\\text{bias}(x) = \\langle(\\bar{g}(x) - f(x))^2\\rangle_D $ is due to the limitations of $H$. \n",
    "\n",
    "Finally, taking the $x$ average:\n",
    "\n",
    "$$ \\langle E_{out}(g^D \\rangle_D = \\langle \\text{bias}(x) + \\text{var}(x) \\rangle_x = \\text{bias} + \\text{var} $$\n",
    "\n",
    "The \"tradeoff\" is that as the complexity of $H$ increases, the bias gets smaller, but the variance gets bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
